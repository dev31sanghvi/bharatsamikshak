{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdXEqvd4ZlN5"
      },
      "source": [
        "## Part 1\n",
        "run the entire part directly no changes needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "212hm9hG-zS-",
        "outputId": "a486d5fa-435c-4347-f47e-d25843e0947e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BharatShamikshak'...\n",
            "remote: Enumerating objects: 171, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 171 (delta 38), reused 88 (delta 22), pack-reused 53\u001b[K\n",
            "Receiving objects: 100% (171/171), 765.30 KiB | 8.06 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "/content/BharatShamikshak/llm-server\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shrutsureja/BharatShamikshak.git\n",
        "%cd BharatShamikshak/llm-server/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAFloP6H_24h",
        "outputId": "7f00b162-d8df-486d-bf31-a35dc0577a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1 (from -r requirements.txt (line 1))\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flask_ngrok (from -r requirements.txt (line 2))\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Collecting flask-cors (from -r requirements.txt (line 3))\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Collecting langchain==0.0.274 (from -r requirements.txt (line 4))\n",
            "  Downloading langchain-0.0.274-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpt4all==1.0.8 (from -r requirements.txt (line 5))\n",
            "  Downloading gpt4all-1.0.8-py3-none-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.4.7 (from -r requirements.txt (line 6))\n",
            "  Downloading chromadb-0.4.7-py3-none-any.whl (415 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.5/415.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python==0.1.81 (from -r requirements.txt (line 7))\n",
            "  Downloading llama_cpp_python-0.1.81.tar.gz (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWuLim6ZHllp",
        "outputId": "4a56e3a6-10a1-4289-ca04-b083ea728a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok version 3.10.0\n",
            "pyngrok version 7.1.6\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyngrok\n",
        "!ngrok --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKemn61hErDc",
        "outputId": "3ff3f48f-1e18-4790-8822-d2d426a74cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m235.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m310.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp310-cp310-linux_x86_64.whl size=80964337 sha256=77b18069ba21d410aa4739629efcb1c04ebac0d67e5849311b7ec5dbff100bf3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n16f9xk0/wheels/a0/e5/04/a5fa9e60033548f205f0db5f6ab6f59cd27bd0da7f9c51cfe7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.1.81\n",
            "    Uninstalling llama_cpp_python-0.1.81:\n",
            "      Successfully uninstalled llama_cpp_python-0.1.81\n",
            "Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.76 numpy-1.26.4 typing-extensions-4.12.0\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TxSPkuU2_3HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bcc472-92af-45e0-943d-8197253648d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'SSIP'\n",
            "/content/SSIP\n"
          ]
        }
      ],
      "source": [
        "# only if you have restarted the server\n",
        "%cd BharatShamikshak/llm-server/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gZOT2HrjFYxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030d4dca-f045-4ae0-94af-d5fdbad166f3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SSIP/models\n",
            "--2024-05-25 00:38:37--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.88, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1716856717&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjg1NjcxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=rvaPV6dYs3XEoeKpMDxa16UnvFbk3tAn2LKyYQdxwiuia1aBAxt-GzF0hnprqJuR4NBeZyRsaB4yADL5cpFIk8oCd0nmRPiRz-Nk8TODMEzoQAJ8SBZaHkGWQ2DcxVcwbsFJHSnrdKBSOk7wG5d8jcKcJZgWmXBBYKKZhvcH8BAehgRinm%7EBKKauZaGrgXp9GoYLI8UbapVQUZU3yjdCmQI2bD7WsB95wTOvHFszsY-Lufo%7EhJsWI3Q9lyi4kCgR0djL4EBcm-zuwN9L8iv1S7k1O91uxJIzbNofs2T8EKdYhKlhZ3FBjuG7rD8fDeBbu17lCpStO68bKVxUn1bsaQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-05-25 00:38:37--  https://cdn-lfs.huggingface.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1716856717&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjg1NjcxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=rvaPV6dYs3XEoeKpMDxa16UnvFbk3tAn2LKyYQdxwiuia1aBAxt-GzF0hnprqJuR4NBeZyRsaB4yADL5cpFIk8oCd0nmRPiRz-Nk8TODMEzoQAJ8SBZaHkGWQ2DcxVcwbsFJHSnrdKBSOk7wG5d8jcKcJZgWmXBBYKKZhvcH8BAehgRinm%7EBKKauZaGrgXp9GoYLI8UbapVQUZU3yjdCmQI2bD7WsB95wTOvHFszsY-Lufo%7EhJsWI3Q9lyi4kCgR0djL4EBcm-zuwN9L8iv1S7k1O91uxJIzbNofs2T8EKdYhKlhZ3FBjuG7rD8fDeBbu17lCpStO68bKVxUn1bsaQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.94, 18.154.185.26, 18.154.185.27, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368438944 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   4.07G   205MB/s    in 28s     \n",
            "\n",
            "2024-05-25 00:39:05 (147 MB/s) - ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’ saved [4368438944/4368438944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir models\n",
        "%cd models\n",
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NULL2nQXGE9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61b5739-152a-4349-e89a-211263b7c234"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SSIP\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qDzy8r_ngn_M"
      },
      "outputs": [],
      "source": [
        "!mv ex.env .env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf-WaivXZDqL"
      },
      "source": [
        "## Part 2 (Server)\n",
        "Main part is api.py run this cell and the RAG server is on\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO3VfH9f9ShX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd6Xb6k889WP",
        "outputId": "9efcac8f-0bbf-4bac-9956-f647cbf6927a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: ngrok 1.2.0\n",
            "Uninstalling ngrok-1.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/ngrok-asgi\n",
            "    /usr/local/lib/python3.10/dist-packages/ngrok-1.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/ngrok/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled ngrok-1.2.0\n",
            "Collecting ngrok\n",
            "  Using cached ngrok-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "Installing collected packages: ngrok\n",
            "Successfully installed ngrok-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# prompt: how can i uninstall the ngrok and then install the latest version\n",
        "\n",
        "!pip uninstall ngrok\n",
        "!pip install --upgrade ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHmayQlG9nfJ"
      },
      "outputs": [],
      "source": [
        "!ngrok --version\n",
        "!python load_document.py\n",
        "!python ragFunction.py\n",
        "!python chatLLM.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10TdhqmtVYSu",
        "outputId": "a8390b02-2951-42bb-ff9e-f3eb1186c33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            " * ngrok tunnel \"https://d8f2-34-136-84-152.ngrok-free.app\" -> \"http://127.0.0.1:5000/\"\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "127.0.0.1 - - [25/May/2024 01:02:58] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [25/May/2024 01:02:58] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [25/May/2024 01:03:52] \"OPTIONS /ingest HTTP/1.1\" 200 -\n",
            "Ingesting...\n",
            "Loading Embeddings...\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "Embeddings loaded in 3.81 s.\n",
            "embed :  client=INSTRUCTOR(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
            "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
            "  (3): Normalize()\n",
            ") model_name='hkunlp/instructor-large' cache_folder=None model_kwargs={} encode_kwargs={} embed_instruction='Represent the document for retrieval: ' query_instruction='Represent the question for retrieving supporting documents: '\n",
            "Creating Embeddings...\n",
            "Creating new vectorstore\n",
            "Loading documents from source_documents\n",
            "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 18.50it/s]\n",
            "Loaded 7 new documents from source_documents\n",
            "Split into 31 chunks of text (max. 600 tokens each)\n",
            "Ingestion Done in 2.98 s.\n",
            "Ingestion complete! Vectorstore stored at db\n",
            "Ingestion Done\n",
            "127.0.0.1 - - [25/May/2024 01:04:00] \"GET /ingest HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [25/May/2024 01:04:26] \"OPTIONS /rag?query=How+many+team+members+can+take+part+in+this+hackathon+?+ HTTP/1.1\" 200 -\n",
            "Embeddings already loaded\n",
            "embed :  client=INSTRUCTOR(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
            "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
            "  (3): Normalize()\n",
            ") model_name='hkunlp/instructor-large' cache_folder=None model_kwargs={} encode_kwargs={} embed_instruction='Represent the document for retrieval: ' query_instruction='Represent the question for retrieving supporting documents: '\n",
            "Loading LLM...\n",
            "LLM loaded in 1.62 s.\n",
            "\n",
            "llm :  \u001b[1mLlamaCpp\u001b[0m\n",
            "Params: {'model_path': 'models/mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'suffix': None, 'max_tokens': 256, 'temperature': 0.8, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop_sequences': [], 'repeat_penalty': 1.1, 'top_k': 40}\n",
            " Each team must have a minimum of 6 members including the team leader, and at least one female team member.\n",
            "> Question :How many team members can take part in this hackathon ? \n",
            "\n",
            "> Answer : (took 1.4 s.)\n",
            " Each team must have a minimum of 6 members including the team leader, and at least one female team member.\n",
            "Answer length is :  107\n",
            "\n",
            "> Source Documents : 12. Tag official page of innovation cell, ministry of education on twitter \n",
            " \n",
            "Please note that the internal hackathon can be organized either in online or offline mode. \n",
            " \n",
            "TEAM FORMATION \n",
            " \n",
            "1) All team members should be from same college; no inter-college teams are allowed. However, \n",
            "members from different branches of the same college/ institute are encouraged to form a team. \n",
            "2) Each team would mandatorily comprise of 6 members including the team leader. \n",
            "3) Each team must have AT LEAST ONE FEMALE TEAM MEMBER.\n",
            "RAG COMPLETED\n",
            "Answer is :   Each team must have a minimum of 6 members including the team leader, and at least one female team member.\n",
            "Source Document is :  12. Tag official page of innovation cell, ministry of education on twitter \n",
            " \n",
            "Please note that the internal hackathon can be organized either in online or offline mode. \n",
            " \n",
            "TEAM FORMATION \n",
            " \n",
            "1) All team members should be from same college; no inter-college teams are allowed. However, \n",
            "members from different branches of the same college/ institute are encouraged to form a team. \n",
            "2) Each team would mandatorily comprise of 6 members including the team leader. \n",
            "3) Each team must have AT LEAST ONE FEMALE TEAM MEMBER.\n",
            "Time taken is :  1.4\n",
            "127.0.0.1 - - [25/May/2024 01:04:29] \"GET /rag?query=How+many+team+members+can+take+part+in+this+hackathon+?+ HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [25/May/2024 01:04:58] \"OPTIONS /rag?query=What+are+the+themes+in+this+hackathon+? HTTP/1.1\" 200 -\n",
            "Embeddings already loaded\n",
            "embed :  client=INSTRUCTOR(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
            "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
            "  (3): Normalize()\n",
            ") model_name='hkunlp/instructor-large' cache_folder=None model_kwargs={} encode_kwargs={} embed_instruction='Represent the document for retrieval: ' query_instruction='Represent the question for retrieving supporting documents: '\n",
            "Loading LLM...\n",
            "LLM loaded in 0.0 s.\n",
            "\n",
            "llm :  \u001b[1mLlamaCpp\u001b[0m\n",
            "Params: {'model_path': 'models/mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'suffix': None, 'max_tokens': 256, 'temperature': 0.8, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop_sequences': [], 'repeat_penalty': 1.1, 'top_k': 40}\n",
            " There are no specific themes mentioned in the context given, but the hackathon is a digital product development competition and encourages multi-disciplinary teams with a good mix of Mechanical Engineers, Electronic Engineers, Product Designers, and Programmers. The shortlisted ideas will be evaluated based on sustainability, scale of impact, user experience, and potential for future work progression.\n",
            "> Question :What are the themes in this hackathon ?\n",
            "\n",
            "> Answer : (took 2.95 s.)\n",
            " There are no specific themes mentioned in the context given, but the hackathon is a digital product development competition and encourages multi-disciplinary teams with a good mix of Mechanical Engineers, Electronic Engineers, Product Designers, and Programmers. The shortlisted ideas will be evaluated based on sustainability, scale of impact, user experience, and potential for future work progression.\n",
            "Answer length is :  405\n",
            "\n",
            "> Source Documents : Guidelines and Process flow for SIH Sr. \n",
            " \n",
            " \n",
            " \n",
            "4) As the software edition of the hackathon is digital product development competition, majority of \n",
            "the team members MUST be well versed with programming skills. For the hardware edition, we \n",
            "encourage multi-disciplinary teams – which means your team should have a good mix of \n",
            "Mechanical Engineers, Electronic Engineers, Product Designers and Programmers, etc. \n",
            "5) The nominated teams should have participated in the recommended Internal hackathon. \n",
            " \n",
            "TEAM SUBMISSION PROCESS\n",
            "RAG COMPLETED\n",
            "Answer is :   There are no specific themes mentioned in the context given, but the hackathon is a digital product development competition and encourages multi-disciplinary teams with a good mix of Mechanical Engineers, Electronic Engineers, Product Designers, and Programmers. The shortlisted ideas will be evaluated based on sustainability, scale of impact, user experience, and potential for future work progression.\n",
            "Source Document is :  Guidelines and Process flow for SIH Sr. \n",
            " \n",
            " \n",
            " \n",
            "4) As the software edition of the hackathon is digital product development competition, majority of \n",
            "the team members MUST be well versed with programming skills. For the hardware edition, we \n",
            "encourage multi-disciplinary teams – which means your team should have a good mix of \n",
            "Mechanical Engineers, Electronic Engineers, Product Designers and Programmers, etc. \n",
            "5) The nominated teams should have participated in the recommended Internal hackathon. \n",
            " \n",
            "TEAM SUBMISSION PROCESS\n",
            "Time taken is :  2.95\n",
            "127.0.0.1 - - [25/May/2024 01:05:01] \"GET /rag?query=What+are+the+themes+in+this+hackathon+? HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [25/May/2024 01:05:55] \"OPTIONS /rag?query=what+are+the+benifites+of+me+participating+in+this+hackathon+?+ HTTP/1.1\" 200 -\n",
            "Embeddings already loaded\n",
            "embed :  client=INSTRUCTOR(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
            "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
            "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
            "  (3): Normalize()\n",
            ") model_name='hkunlp/instructor-large' cache_folder=None model_kwargs={} encode_kwargs={} embed_instruction='Represent the document for retrieval: ' query_instruction='Represent the question for retrieving supporting documents: '\n",
            "Loading LLM...\n",
            "LLM loaded in 0.0 s.\n",
            "\n",
            "llm :  \u001b[1mLlamaCpp\u001b[0m\n",
            "Params: {'model_path': 'models/mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'suffix': None, 'max_tokens': 256, 'temperature': 0.8, 'top_p': 0.95, 'logprobs': None, 'echo': False, 'stop_sequences': [], 'repeat_penalty': 1.1, 'top_k': 40}\n",
            " There are several benefits of participating in this hackathon, including the opportunity to develop new skills, network with other professionals in your field, showcase your work to potential employers, and potentially receive funding for your project. Additionally, the grand finale will provide you with the chance to travel to a nodal center and meet with other teams and mentors.\n",
            "> Question :what are the benifites of me participating in this hackathon ? \n",
            "\n",
            "> Answer : (took 2.71 s.)\n",
            " There are several benefits of participating in this hackathon, including the opportunity to develop new skills, network with other professionals in your field, showcase your work to potential employers, and potentially receive funding for your project. Additionally, the grand finale will provide you with the chance to travel to a nodal center and meet with other teams and mentors.\n",
            "Answer length is :  384\n",
            "\n",
            "> Source Documents : Guidelines and Process flow for SIH Sr. \n",
            " \n",
            " \n",
            " \n",
            "4) As the software edition of the hackathon is digital product development competition, majority of \n",
            "the team members MUST be well versed with programming skills. For the hardware edition, we \n",
            "encourage multi-disciplinary teams – which means your team should have a good mix of \n",
            "Mechanical Engineers, Electronic Engineers, Product Designers and Programmers, etc. \n",
            "5) The nominated teams should have participated in the recommended Internal hackathon. \n",
            " \n",
            "TEAM SUBMISSION PROCESS\n",
            "RAG COMPLETED\n",
            "Answer is :   There are several benefits of participating in this hackathon, including the opportunity to develop new skills, network with other professionals in your field, showcase your work to potential employers, and potentially receive funding for your project. Additionally, the grand finale will provide you with the chance to travel to a nodal center and meet with other teams and mentors.\n",
            "Source Document is :  Guidelines and Process flow for SIH Sr. \n",
            " \n",
            " \n",
            " \n",
            "4) As the software edition of the hackathon is digital product development competition, majority of \n",
            "the team members MUST be well versed with programming skills. For the hardware edition, we \n",
            "encourage multi-disciplinary teams – which means your team should have a good mix of \n",
            "Mechanical Engineers, Electronic Engineers, Product Designers and Programmers, etc. \n",
            "5) The nominated teams should have participated in the recommended Internal hackathon. \n",
            " \n",
            "TEAM SUBMISSION PROCESS\n",
            "Time taken is :  2.71\n",
            "127.0.0.1 - - [25/May/2024 01:05:58] \"GET /rag?query=what+are+the+benifites+of+me+participating+in+this+hackathon+?+ HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTpTnE_FaErY"
      },
      "source": [
        "## Part 3 (Debug Section)\n",
        "#### not to be **USED**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQz13VcPYv2b"
      },
      "source": [
        "### Loading this cell is not necessary but if it makes any error then load this cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QlnrTCnDd9Z",
        "outputId": "53126501-55c2-4f5a-b61c-7d6ec47f6b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.136.84.152\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tecsXP9n_3QN",
        "outputId": "2e3c8d83-2afe-4522-9724-a02f399607ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/load_document.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python load_document.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v2eTKKfvVNua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98139d41-a910-41c5-87f5-db0b7ce41aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/embeddingsload.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python embeddingsload.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f6cIz-h6A7uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7220eeb-58d3-4f18-f71c-5cf2129bea74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/llmload.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python llmload.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "prvOP4L0j0Zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d290b6-09aa-4390-ad4e-8141f4ac3e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/ragFunction.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python ragFunction.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uCXbROzhmOQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f0f3e2-c615-499d-facc-e64923369ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/chatLLM.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python chatLLM.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCKYc7yDP5QS",
        "outputId": "c27f044d-0043-4541-ca54-1b92f1e950ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/dummyapi.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python dummyapi.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWY49CO4ZT9m"
      },
      "source": [
        "### Rough Section for debuging not to be **USED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmQJxDnWqbNZ",
        "outputId": "ab27aaab-1011-40a6-f69f-d60f0719cef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "rm: cannot remove 'SSIP': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%rm -r SSIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6Q1snP_0VgwY"
      },
      "outputs": [],
      "source": [
        "#both the documents in the SIH and SSIP are uploaded\n",
        "#now ask the question that \"is all boys team allowed in SSIP?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbImB71wsSas",
        "outputId": "63411aa1-1b0d-48a7-ed02-558a64816702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSIP\n",
            "apiOLD.py\t       constants.py\t    llmload.py\t      ragFunction.py\tSS_RAG.ipynb\n",
            "app.py\t\t       dummyapi.py\t    load_document.py  README.md\n",
            "chatLLM.py\t       embeddingsload.py    models\t      requirements.txt\n",
            "chat_with_document.py  GenerateQuestion.py  __pycache__       source_documents\n"
          ]
        }
      ],
      "source": [
        "%cd SSIP\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYzqWu4Yw-Gs",
        "outputId": "3bcf2b65-a917-4105-92f6-d0d7634fb178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '//chat_with_document.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python chat_with_document.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wTpTnE_FaErY",
        "CQz13VcPYv2b"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}